{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Mostly from https://github.com/kingoflolz/mesh-transformer-jax\n","# And torch loader stuff from https://github.com/KoboldAI/KoboldAI-Client tpu_mtj_backend.py and torch_lazy_loader.py\n","# So probably under AGPL and apache mix"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Example of using pytorch lazy loader to create a checkpoint then load it into a TPU without using RAM directly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWAGjGr785xX"},"outputs":[],"source":["# !rm pytorch_model.bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cu5PVvct3Bpm"},"outputs":[],"source":["!time wget -c https://huggingface.co/EleutherAI/gpt-neo-2.7B/resolve/main/pytorch_model.bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYMTKCF3AuZJ"},"outputs":[],"source":["# !time wget -c https://huggingface.co/EleutherAI/gpt-neo-1.3B/resolve/main/pytorch_model.bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCJ9RuNMh-OC"},"outputs":[],"source":["!wget -c https://github.com/henk717/KoboldAI/raw/united/requirements_mtj.txt\n","!wget -c https://github.com/henk717/KoboldAI/raw/united/torch_lazy_loader.py\n","!wget -c https://github.com/henk717/KoboldAI/raw/united/utils.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-KXx6OdGtKm"},"outputs":[],"source":["# !pip install -r /content/requirements_mtj.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9FnK64FyZQk"},"outputs":[],"source":["!pip install numpy tqdm requests optax==0.0.9 dm-haiku==0.0.9 chex==0.1.5 jax==0.3.25 jaxlib==0.3.25 transformers progressbar2 git+https://github.com/Zurnaz/mesh-transformer-jax.git@tpu_driver"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Allows running locally on 8 devices using CPU vs TPU\n","# import os\n","# os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EBDzNAaVYDO"},"outputs":[],"source":["\n","import os\n","import requests \n","from jax.config import config\n","\n","driver_version=\"tpu_driver_nightly\"\n","# driver_version=\"tpu_driver_20221011\"\n","#  driver_version=\"tpu_driver0.2\"\n","if os.environ.get('COLAB_TPU_ADDR', '') != '':\n","    tpu_address = os.environ['COLAB_TPU_ADDR']  # Colab\n","else:\n","    tpu_address = os.environ['TPU_NAME']  # Kaggle\n","\n","tpu_address = tpu_address.replace(\"grpc://\", \"\")\n","tpu_address_without_port = tpu_address.split(':', 1)[0]\n","url = f'http://{tpu_address_without_port}:8475/requestversion/{driver_version}'\n","requests.post(url)\n","\n","\n","# The following is required to use TPU Driver as JAX's backend.\n","config.FLAGS.jax_xla_backend = \"tpu_driver\"\n","config.FLAGS.jax_backend_target = \"grpc://\" + tpu_address\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tg7nWW1AydGk"},"outputs":[],"source":["mtj_config_1 = {\n","  \"compat\": \"neo\",\n","  \"pe\": \"fixed\",\n","  \"layers\": 32,\n","  \"d_model\": 2560, \n","  \"n_heads\": 20,\n","  \"n_vocab\": 50257,\n","  \"n_vocab_padding\": 0,\n","  \"norm\": \"layernorm\",\n","  # \"pe_rotary_pct\": 0.25,\n","  # \"pe_rotary_dims\": 64,\n","  # \"d_head\": 256,\n","  \"seq\": 2048,\n","  \"cores_per_replica\": 4,\n","  \"per_replica_batch\": 1,\n","  # \"d_embed\": 5120,\n","  # \"early_cast\": True,\n","  \"do_layer_norm_before\": True,\n","}\n","\n","mtj_config_2 = {\n","  \"layers\": 28,\n","  \"d_model\": 4096,\n","  \"n_heads\": 16,\n","  \"n_vocab\": 50400,\n","  \"norm\": \"layernorm\",\n","  \"pe\": \"rotary\",\n","  \"pe_rotary_dims\": 64,\n","  \"d_head\": 256,\n","  \"seq\": 2048,\n","  \"cores_per_replica\": 8,\n","  \"per_replica_batch\": 1,\n","}\n","mtj_config = mtj_config_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOYM5mwxzGKt"},"outputs":[],"source":["mtj_config[\"n_vocab_padding\"] = -(mtj_config[\"n_vocab\"] % -mtj_config[\"cores_per_replica\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ResKZwA8QZbW"},"outputs":[],"source":["checkpoint_dir = \"jax_checkpoint\" # jax_checkpoint step_383500"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKseshGm1bzO"},"outputs":[],"source":["total_shards = mtj_config[\"cores_per_replica\"]\n","d_model = mtj_config[\"d_model\"]\n","layers = mtj_config[\"layers\"]\n","pieces = 16\n","padding_rows = mtj_config[\"n_vocab_padding\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Z06bPXbxYZ-"},"outputs":[],"source":["#@title Load checkpoint\n","config_path = \"config.json\"\n","work_dir = \"\" # \"/content/\"\n","checkpoint_dir = \"jax_checkpoint\" # jax_checkpoint step_383500\n","path_to_checkpoint = f\"{work_dir}pytorch_model.bin\" \n","import torch_lazy_loader\n","import os\n","from termcolor import colored\n","from IPython.display import clear_output\n","import torch\n","  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQfagT9txcky"},"outputs":[],"source":["#@title Convert checkpoint to be JAX-compatible\n","import zipfile\n","import functools\n","from IPython.display import clear_output\n","import torch\n","import numpy as np\n","import jax.numpy as jnp\n","\n","def callback(model_dict, f, **_):\n","  for i in range(total_shards):\n","      os.makedirs(f\"{checkpoint_dir}/shard_{i}\")\n","  pieces = 16\n","\n","  def reshard_reverse(x, old_shape, is_shard_bias=False):\n","      if len(x.shape) == 1:\n","          assert False\n","          out = x[0:1]\n","\n","      elif len(x.shape) == 2:\n","          #print(f\"LN/bias\")\n","          if old_shape[1] == x.shape[1]:\n","              #print(\"LN\")\n","              if not is_shard_bias:\n","                  out = np.tile(x[0:1], (total_shards, 1))\n","              else:\n","                  #print(\"shard bias\")\n","                  out = np.tile(x[0:1], (total_shards, 1)) / total_shards\n","          else:\n","              #print(\"bias\")\n","              out = x.reshape(old_shape)\n","\n","      elif len(x.shape) == 3:\n","          if x.shape[0] * x.shape[2] == old_shape[2]:\n","              #print(\"case 1\")\n","              out = x.reshape(old_shape)\n","          elif x.shape[0] * x.shape[1] == old_shape[1]:\n","              #print(\"case 2\")\n","              out = jnp.transpose(x.reshape((old_shape[1], old_shape[0], old_shape[2])), (1, 0, 2))\n","          else:\n","              raise Exception(f\"unimplemented, {x.shape}, {old_shape}\")\n","      else:\n","          raise Exception(f\"unimplemented, {x}\")\n","      #flattened, structure =jax.tree_util.tree_structure(out)\n","      #return flattened\n","      return out\n","\n","  def get_old_shape(t, dim=2):\n","      if len(t.shape) == 2:\n","          shard_shape = t.shape\n","          if dim == 1:\n","              assert shard_shape[0] % total_shards == 0\n","              return (shard_shape[0] // total_shards, shard_shape[1])\n","          elif dim == 2:\n","              assert shard_shape[1] % total_shards == 0\n","              return (shard_shape[0], shard_shape[1] // total_shards)\n","          else:\n","              raise ValueError(f\"unsupported dim {dim}\")\n","      if len(t.shape) == 1:\n","          assert t.shape[0] % total_shards == 0\n","          return (t.shape[0] // total_shards,)\n","      else:\n","          raise ValueError(f\"unsupported shape {t.shape}\")\n","\n","\n","  def split(a, n):\n","      k, m = divmod(len(a), n)\n","      return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n","\n","  def save(cpu_flattened):\n","      for i in range(total_shards):\n","          cpu_flattened_chunked = split(cpu_flattened, pieces)\n","          for j, chunk in enumerate(cpu_flattened_chunked):\n","              with open(f\"{checkpoint_dir}/shard_{i}/{j}.npz\", \"wb\") as f:\n","                  np.savez(f, *map(lambda c: c[i], chunk))\n","\n","\n","  transforms = [\n","      (\"transformer.wpe.weight\", False, 2),\n","      (\"transformer.wte.weight\", False, 1)\n","  ]\n","  \n","  layer_names = sorted(map(str, range(layers)))\n","  for layer in layer_names:\n","      transforms.extend([\n","          (f\"transformer.h.{layer}.attn.attention.q_proj.weight\", False, 2),\n","          (f\"transformer.h.{layer}.attn.attention.v_proj.weight\", False, 2),\n","          (f\"transformer.h.{layer}.attn.attention.k_proj.weight\", False, 2),\n","          (f\"transformer.h.{layer}.attn.attention.out_proj.bias\", True, None),\n","          (f\"transformer.h.{layer}.attn.attention.out_proj.weight\", False, 1),\n","          (f\"transformer.h.{layer}.mlp.c_fc.bias\", False, 1),\n","          (f\"transformer.h.{layer}.mlp.c_fc.weight\", False, 2),\n","          (f\"transformer.h.{layer}.mlp.c_proj.bias\", True, None),\n","          (f\"transformer.h.{layer}.mlp.c_proj.weight\", False, 1),\n","          (f\"transformer.h.{layer}.ln_1.bias\", False, None),\n","          (f\"transformer.h.{layer}.ln_1.weight\", False, None),\n","          (f\"transformer.h.{layer}.ln_2.bias\", False, None),\n","          (f\"transformer.h.{layer}.ln_2.weight\", False, None),\n","      ])\n","\n","  transforms.extend([\n","      (\"transformer.ln_f.bias\", False, None),\n","      (\"transformer.ln_f.weight\", False, None),\n","  ])\n","  \n","  checkpoint = []\n","  with zipfile.ZipFile(f, \"r\") as z:\n","    last_storage_key = None\n","    zipfolder = os.path.basename(os.path.normpath(f)).split('.')[0]\n","    f = None\n","    current_offset = 0\n","    for i in range(len(transforms)):\n","        transform = transforms.pop(0)\n","        print(i, transform[0])\n","        \n","        key = transform[0]\n","\n","        storage_key = model_dict[key].key\n","        if storage_key != last_storage_key or model_dict[key].seek_offset < current_offset:\n","            last_storage_key = storage_key\n","            if isinstance(f, zipfile.ZipExtFile):\n","                f.close()\n","            try:\n","                f = z.open(f\"archive/data/{storage_key}\")\n","            except:\n","                f = z.open(f\"{zipfolder}/data/{storage_key}\")\n","            current_offset = 0\n","        if current_offset != model_dict[key].seek_offset:\n","            f.read(model_dict[key].seek_offset - current_offset)\n","            current_offset = model_dict[key].seek_offset\n","        if not isinstance(model_dict[key], torch_lazy_loader.LazyTensor):\n","            error = f\"Duplicate key {repr(key)}\"\n","            print(\"\\n\\nERROR:  \" + error, file=sys.stderr)\n","            raise RuntimeError(error)\n","        size = functools.reduce(lambda x, y: x * y, model_dict[key].shape, 1)\n","        dtype = model_dict[key].dtype\n","        nbytes = size if dtype is torch.bool else size * ((torch.finfo if dtype.is_floating_point else torch.iinfo)(dtype).bits >> 3)\n","        tensor = model_dict[key].materialize(f, map_location=\"cpu\")\n","        model_dict[key] = tensor.to(\"meta\")\n","        current_offset += nbytes\n","\n","        params = tensor\n","\n","        # Pad input and output embeddings with 0 at the bottom to have 50400 rows\n","        # instead of 50257 rows (the padding value doesn't have to be 0, it doesn't\n","        # even have to be a constant value; the only thing the padding affects is\n","        # it adds junk logits to the end of the logits array the transformer returns\n","        # without affecting the other logits)\n","        if transform[0] in (\"transformer.wte.weight\", \"lm_head.weight\"):\n","            params = torch.cat((params, torch.zeros(padding_rows, params.shape[-1], device=params.device)), dim=0)\n","            # params = torch.cat((params, torch.zeros(143, params.shape[1])))\n","        # torch.nn.Linear uses a transposed version of the equivalent tensor that\n","        # haiku.Linear uses, so we have to un-transpose the tensor first\n","        if not any(s in transform[0] for s in (\"wte\", \"wpe\")):\n","            params = params.T\n","            \n","        \n","        if transform[2] is not None:\n","            old_shape = (total_shards,) + get_old_shape(params, transform[2])\n","        else:\n","            old_shape = (total_shards, params.shape[0],)\n","        \n","        # print(f\"<1 [{transform[0]}] {params.shape} to {old_shape}\")\n","        \n","        params = np.asarray(params[None], dtype=jnp.bfloat16)\n","        params = reshard_reverse(params, old_shape, is_shard_bias=transform[1])\n","        \n","        if np.isnan(params).any() or np.isinf(params).any():\n","            raise ValueError(f\"bfloat16 overflow/underflow\")\n","\n","        #print(f\">2 [{transform[0]}] {params.shape}\")\n","        assert params.shape == old_shape\n","        checkpoint.append(params)\n","\n","  # Append the checkpoint step number (can be set to an arbitrary value, in this\n","  # case 0, as long as we're only using inference and not training the model)\n","  checkpoint.append(np.zeros(total_shards, dtype=np.int32))\n","\n","  print(\"saving\")\n","  save(checkpoint)\n","  del checkpoint\n","  del params\n","  print(colored(f\"DONE! The JAX checkpoint is now stored at {work_dir}{checkpoint_dir}\", \"green\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OM8SyDHXCiYT"},"outputs":[],"source":["!rm -R jax_checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBBBP7PtnywJ"},"outputs":[],"source":["\n","with torch_lazy_loader.use_lazy_torch_load(callback=callback, dematerialized_modules=True):\n","  torch_checkpoint = torch.load(path_to_checkpoint, map_location=\"cpu\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ro8faMarrA6I"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fj9JaWjWyULN"},"outputs":[],"source":["import time\n","\n","import jax\n","from jax.experimental import maps\n","import numpy as np\n","import optax\n","import transformers\n","\n","from mesh_transformer.checkpoint import read_ckpt_lowmem, read_ckpt\n","from mesh_transformer.sampling import nucleaus_sample\n","from mesh_transformer.transformer_shard import CausalTransformer, CausalTransformerV2\n","\n","per_replica_batch = mtj_config[\"per_replica_batch\"]\n","cores_per_replica = mtj_config[\"cores_per_replica\"]\n","seq = mtj_config[\"seq\"]\n","\n","\n","mtj_config[\"sampler\"] = nucleaus_sample\n","\n","# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n","# mtj_config[\"optimizer\"] = optax.scale(0)\n","\n","mesh_shape = (1, cores_per_replica)\n","devices = jax.devices()\n","devices = np.array(devices[:cores_per_replica]).reshape(mesh_shape)\n","\n","# mesh_shape = ( jax.device_count() // cores_per_replica, cores_per_replica)\n","# devices = np.array(jax.devices()).reshape(mesh_shape)\n","\n","print(\"mesh_shape\", mesh_shape)\n","print(\"devices\", devices)\n","\n","# maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp','mp')),())\n","global_mesh = maps.Mesh(devices, ('dp', 'mp'))\n","maps.thread_resources.env = maps.ResourceEnv(physical_mesh=global_mesh, loops=())\n","\n","tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n","\n","total_batch = per_replica_batch * jax.device_count() // cores_per_replica"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcggOpWoyJYb"},"outputs":[],"source":["from typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple, TypeVar\n","class _EmptyState(NamedTuple):\n","    pass\n","\n","class _DummyOptimizer:\n","    def init(*args, **kwargs):\n","        return _EmptyState()\n","mtj_config[\"optimizer\"] = _DummyOptimizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mv_ewWme2fEw"},"outputs":[],"source":["# network = CausalTransformer(mtj_config)\n","network = CausalTransformer(mtj_config, dematerialized=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from mesh_transformer.util import to_bf16, to_f16, to_f32\n","move_xmap = jax.experimental.maps.xmap(\n","    fun=lambda x, _: to_f32(x),\n","    in_axes=([\"shard\", ...], [\"batch\", ...]),\n","    out_axes=[\"shard\", ...],\n","    axis_resources={'shard': 'mp', 'batch': 'dp'}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbpszoqONZzx"},"outputs":[],"source":["from mesh_transformer.checkpoint import read_ckpt_lowmem, read_ckpt\n","# network.state[\"opt_state\"] = None # only used when re-running block to reset network\n","network.state = read_ckpt(network.state, f\"{checkpoint_dir}/\", cores_per_replica, shards_out=cores_per_replica)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["network.state = move_xmap(network.state, np.zeros(cores_per_replica))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ikzz4b9P2i6R"},"outputs":[],"source":["def infer(context, top_p=0.9, temp=1.0, gen_len=512):\n","    tokens = tokenizer.encode(context)\n","\n","    provided_ctx = len(tokens)\n","    pad_amount = seq - provided_ctx\n","\n","    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n","    batched_tokens = np.array([padded_tokens] * total_batch)\n","    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n","\n","    start = time.time()\n","    output = network.generate(\n","        batched_tokens,\n","        length,\n","        gen_len,\n","        {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp}\n","    )\n","\n","    samples = []\n","    decoded_tokens = output[1][0]\n","    # print(\"output\", len(output[0]))\n","    for o in decoded_tokens[:, :, 0]:\n","      samples.append(tokenizer.decode(o))\n","\n","      #samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\n","        # single = o[0][0, 0, seq : seq + gen_len]\n","        # print(\"single\", single, tokenizer.decode(single))\n","    print(f\"completion done in {time.time() - start:06}s\")\n","    return samples\n","\n","# print(infer(\"EleutherAI is\")[0])\n","# print(infer(\"EleutherAI is\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l82_b0tC2l4d"},"outputs":[],"source":["#@title  { form-width: \"300px\" }\n","top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","#context = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\"\"\n","context = \"\"\"The world of tomorrow is going\"\"\"\n","print(infer(top_p=top_p, temp=temp, gen_len=10, context=context))"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyPZeaPCZAPwTBlC8DhnT014","private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}

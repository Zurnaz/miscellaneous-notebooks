{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Isolated code to load huggingface models into TPU\n",
        "# Mix of AGPL and apache from https://github.com/VE-FORBRYDERNE/mesh-transformer-jax and https://github.com/KoboldAI/KoboldAI-Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAyieL3ZpLpo"
      },
      "outputs": [],
      "source": [
        "\n",
        "!wget -c https://github.com/henk717/KoboldAI/raw/united/requirements_mtj.txt\n",
        "!wget -c https://github.com/henk717/KoboldAI/raw/united/torch_lazy_loader.py\n",
        "!wget -c https://github.com/henk717/KoboldAI/raw/united/utils.py\n",
        "%pip install -r requirements_mtj.txt\n",
        "%pip install numpy tqdm requests optax==0.0.9 dm-haiku==0.0.9 chex==0.1.5 jax==0.3.25 jaxlib==0.3.25 transformers==4.28.0 progressbar2 sentencepiece \n",
        "%pip install git+https://github.com/Zurnaz/mesh-transformer-jax.git@tpu_driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local 8 simultated devices on CPU\n",
        "# uncomment and comment out tpu driver to load  on CPU and swap move_xmap from to_bf16 to to_f32\n",
        "# import os\n",
        "# os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzMXyz21p4jC"
      },
      "outputs": [],
      "source": [
        "# LOADS on TPU\n",
        "import os\n",
        "import requests \n",
        "from jax.config import config\n",
        "\n",
        "driver_version=\"tpu_driver_20221109\"\n",
        "# driver_version=\"tpu_driver_20221011\"\n",
        "#  driver_version=\"tpu_driver0.2\"\n",
        "if os.environ.get('COLAB_TPU_ADDR', '') != '':\n",
        "    tpu_address = os.environ['COLAB_TPU_ADDR']  # Colab\n",
        "else:\n",
        "    tpu_address = os.environ['TPU_NAME']  # Kaggle\n",
        "\n",
        "tpu_address = tpu_address.replace(\"grpc://\", \"\")\n",
        "tpu_address_without_port = tpu_address.split(':', 1)[0]\n",
        "url = f'http://{tpu_address_without_port}:8475/requestversion/{driver_version}'\n",
        "requests.post(url)\n",
        "\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + tpu_address\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbQ-bu4KpiF5"
      },
      "outputs": [],
      "source": [
        "def get_default_params():\n",
        "    return {\n",
        "        # \"sampler\": nucleaus_sample,\n",
        "        \"compat\": \"j\",\n",
        "        \"layers\": 28,\n",
        "        \"d_model\": 4096,\n",
        "        \"n_heads\": 16,\n",
        "        \"n_vocab\": 50400,\n",
        "        \"n_vocab_padding\": 0,\n",
        "        \"norm\": \"layernorm\",\n",
        "        \"pe\": \"rotary\",\n",
        "        \"pe_rotary_dims\": 64,\n",
        "        \"seq\": 2048,\n",
        "        \"cores_per_replica\": 8,\n",
        "        \"tokenizer_class\": \"GPT2Tokenizer\",\n",
        "        \"tokenizer\": \"gpt2\",\n",
        "    }\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkVAV4vMpkwW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "def generate_mtj_config(hf_checkpoint, model_type, params):\n",
        "    default_params = get_default_params()\n",
        "    \n",
        "    if hf_checkpoint:\n",
        "        # Try to convert HF config.json to MTJ config\n",
        "        spec_path = os.path.join(\"maps\", model_type + \".json\")\n",
        "        if not os.path.isfile(spec_path):\n",
        "            raise NotImplementedError(f\"Unsupported model type {repr(model_type)}\")\n",
        "        with open(spec_path) as f:\n",
        "            lazy_load_spec = json.load(f)\n",
        "        \n",
        "        if \"mtj_compat\" in lazy_load_spec:\n",
        "            params[\"compat\"] = lazy_load_spec[\"mtj_compat\"]\n",
        "        if \"mtj_pe\" in lazy_load_spec:\n",
        "            params[\"pe\"] = lazy_load_spec[\"mtj_pe\"]\n",
        "        for k, v in lazy_load_spec.get(\"mtj_config_map\", {}).items():\n",
        "            if type(v) is not list:\n",
        "                params[k] = params[v]\n",
        "                continue\n",
        "            for i in range(len(v)):\n",
        "                if i == len(v) - 1:\n",
        "                    params[k] = v[i]\n",
        "                elif v[i] in params:\n",
        "                    params[k] = params[v[i]]\n",
        "                    break\n",
        "        \n",
        "        params[\"n_vocab\"] = params[\"vocab_size\"]\n",
        "        \n",
        "        if \"activation_function\" in params:\n",
        "            params[\"activation\"] = params[\"activation_function\"]\n",
        "        \n",
        "        # Both the number of attention heads in the model and the embedding\n",
        "        # dimension of the model need to be divisible by the number of TPU cores\n",
        "        # that we use, and JAX also requires the number of TPU cores used to be\n",
        "        # an even number if we're using more than one core, so logically we try\n",
        "        # to pick the largest possible even number of TPU cores such that the\n",
        "        # number of attention heads and embedding dimension are both divisible\n",
        "        # by the number of TPU cores, and fall back to one core if an even\n",
        "        # number of TPU cores is not possible.\n",
        "        for c in (8, 6, 4, 2, 1):\n",
        "            if 0 == params[\"n_heads\"] % c == params.get(\"d_embed\", params[\"d_model\"]) % c:\n",
        "                params[\"cores_per_replica\"] = c\n",
        "                break\n",
        "        \n",
        "        # The vocabulary size of the model also has to be divisible by the\n",
        "        # number of TPU cores, so we pad the vocabulary with the minimum\n",
        "        # possible number of dummy tokens such that it's divisible.\n",
        "        params[\"n_vocab_padding\"] = -(params[\"n_vocab\"] % -params[\"cores_per_replica\"])\n",
        "\n",
        "    if \"compat\" in params:\n",
        "        default_params[\"compat\"] = params[\"compat\"]\n",
        "\n",
        "    if default_params[\"compat\"] == \"fairseq_lm\":\n",
        "        default_params[\"tokenizer\"] = \"KoboldAI/fairseq-dense-125M\"\n",
        "    \n",
        "    for param in default_params:\n",
        "        if param not in params:\n",
        "            params[param] = default_params[param]\n",
        "\n",
        "    # Use an optimization that will allow us to avoid one extra transpose operation\n",
        "    if hf_checkpoint:\n",
        "        params[\"transposed_linear\"] = True\n",
        "    \n",
        "    model_spec = {}\n",
        "    for key, spec in lazy_load_spec.get(\"static_weights\", {}).items():\n",
        "        if spec.get(\"mtj\") is not None:\n",
        "            model_spec[key] = spec[\"mtj\"].copy()\n",
        "            model_spec[key][\"module\"] = \"causal_transformer_shard/~/\" + model_spec[key][\"module\"]\n",
        "    for _key, spec in lazy_load_spec.get(\"layer_weights\", {}).items():\n",
        "        for layer in range(params[\"layers\"]):\n",
        "            if spec.get(\"mtj\") is not None:\n",
        "                key = _key.format(layer=layer)\n",
        "                model_spec[key] = spec[\"mtj\"].copy()\n",
        "                model_spec[key][\"module\"] = \"causal_transformer_shard/~/\" + model_spec[key][\"module\"].format(layer=layer)\n",
        "\n",
        "    return params, model_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Needs the corrisponding maps config for to load the model to generate the config\n",
        "!mkdir maps\n",
        "!wget -c https://raw.githubusercontent.com/henk717/KoboldAI/united/maps/gpt_neo.json -P /content/maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAj6Px32pmTo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name_or_path = 'EleutherAI/gpt-neo-2.7B'\n",
        "\n",
        "hf_config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "\n",
        "mtj_config, model_spec = generate_mtj_config(True, hf_config.model_type, hf_config.__dict__.copy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ETZEMLppt4"
      },
      "outputs": [],
      "source": [
        "mtj_config[\"n_vocab_padding\"] = -(mtj_config[\"n_vocab\"] % -mtj_config[\"cores_per_replica\"])\n",
        "print(\"n_vocab_padding\", mtj_config[\"n_vocab_padding\"] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsevsF_cpqoj"
      },
      "outputs": [],
      "source": [
        "total_shards = mtj_config[\"cores_per_replica\"]\n",
        "d_model = mtj_config[\"d_model\"]\n",
        "layers = mtj_config[\"layers\"]\n",
        "pieces = 16\n",
        "padding_rows = mtj_config[\"n_vocab_padding\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEzjcZGFpsam"
      },
      "outputs": [],
      "source": [
        "import torch_lazy_loader\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x73j3DBpp52n"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import jax\n",
        "from jax.experimental import maps\n",
        "import numpy as np\n",
        "import optax\n",
        "import transformers\n",
        "\n",
        "from mesh_transformer.checkpoint import read_ckpt_lowmem, read_ckpt\n",
        "from mesh_transformer.sampling import nucleaus_sample\n",
        "from mesh_transformer.transformer_shard import CausalTransformer, CausalTransformerV2, PlaceholderTensor\n",
        "\n",
        "per_replica_batch = 1 # mtj_config[\"per_replica_batch\"]\n",
        "cores_per_replica = mtj_config[\"cores_per_replica\"]\n",
        "seq = mtj_config[\"seq\"]\n",
        "\n",
        "\n",
        "mtj_config[\"sampler\"] = nucleaus_sample\n",
        "\n",
        "# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n",
        "mtj_config[\"optimizer\"] = optax.scale(0)\n",
        "\n",
        "mesh_shape = (1, cores_per_replica)\n",
        "devices = jax.devices()\n",
        "devices = np.array(devices[:cores_per_replica]).reshape(mesh_shape)\n",
        "\n",
        "# mesh_shape = ( jax.device_count() // cores_per_replica, cores_per_replica)\n",
        "# devices = np.array(jax.devices()).reshape(mesh_shape)\n",
        "\n",
        "print(\"mesh_shape\", mesh_shape)\n",
        "print(\"devices\", devices)\n",
        "# maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp','mp')),())\n",
        "global_mesh = maps.Mesh(devices, ('dp', 'mp'))\n",
        "maps.thread_resources.env = maps.ResourceEnv(physical_mesh=global_mesh, loops=())\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "# tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "total_batch = per_replica_batch * jax.device_count() // cores_per_replica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LS7sK4Wp7Cz"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple, TypeVar\n",
        "class _EmptyState(NamedTuple):\n",
        "    pass\n",
        "\n",
        "class _DummyOptimizer:\n",
        "    def init(*args, **kwargs):\n",
        "        return _EmptyState()\n",
        "mtj_config[\"optimizer\"] = _DummyOptimizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU1OnA8Jp8vu"
      },
      "outputs": [],
      "source": [
        "# import AutoModelForCausalLM\n",
        "network = CausalTransformer(mtj_config, dematerialized=True)\n",
        "# network = CausalTransformer(mtj_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsLPRmrmdpNF"
      },
      "outputs": [],
      "source": [
        "from mesh_transformer.util import to_bf16, to_f16, to_f32\n",
        "\n",
        "move_xmap = jax.experimental.maps.xmap(\n",
        "    fun=lambda x, _: to_bf16(x),\n",
        "    in_axes=([\"shard\", ...], [\"batch\", ...]),\n",
        "    out_axes=[\"shard\", ...],\n",
        "    axis_resources={'shard': 'mp', 'batch': 'dp'}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FohZky0Ypwcj"
      },
      "outputs": [],
      "source": [
        "#@title Convert checkpoint to be JAX-compatible\n",
        "import sys\n",
        "import zipfile\n",
        "import functools\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax.dlpack\n",
        "\n",
        "def callback(model_dict, f, **_):\n",
        "    # print(\"model_dict\", model_dict.keys())\n",
        "    # print(\"model_spec\", model_spec.keys())\n",
        "    def reshard_reverse(x, old_shape):\n",
        "        assert len(x.shape) != 1\n",
        "        if len(x.shape) == 2:\n",
        "            # print(f\"LN/bias {x.shape}\")\n",
        "            # print(x[:, :4])\n",
        "            if old_shape[1] == x.shape[1]:\n",
        "                out = x[0:1].tile((total_shards, 1))\n",
        "            else:\n",
        "                out = x.reshape(old_shape)\n",
        "            # print(out[:, :4])\n",
        "        elif len(x.shape) == 3:\n",
        "            # print(f\"weight {x.shape}\")\n",
        "            # print(x[:, :4])\n",
        "            if x.shape[0] * x.shape[2] == old_shape[2]:\n",
        "                out = x.reshape(old_shape)\n",
        "            elif x.shape[0] * x.shape[1] == old_shape[1]:\n",
        "                out = x.reshape((old_shape[1], old_shape[0], old_shape[2])).permute((1, 0, 2))\n",
        "            else:\n",
        "                assert False\n",
        "            # print(out[:, :4])\n",
        "        else:\n",
        "            assert False\n",
        "        return out\n",
        "\n",
        "    with zipfile.ZipFile(f, \"r\") as z:\n",
        "        last_storage_key = None\n",
        "        zipfolder = os.path.basename(os.path.normpath(f)).split('.')[0]\n",
        "        f = None\n",
        "        current_offset = 0\n",
        "        def sort_model_dict(k):\n",
        "            return (model_dict[k].key, model_dict[k].seek_offset)\n",
        "        # sorted_keys = sorted(model_dict.keys(), key=lambda k: (model_dict[k].key, model_dict[k].seek_offset))\n",
        "        # sorted_keys = model_spec.keys()\n",
        "        sorted_keys = sorted(map(str, model_dict.keys()))\n",
        "        for i, key in enumerate(sorted_keys):\n",
        "            model_spec_key = max((k for k in model_spec.keys() if key.endswith(k)), key=len, default=None)\n",
        "            print(i, key)\n",
        "            # Some model weights are used by transformers but not by MTJ.\n",
        "            # We have to materialize these weights anyways because\n",
        "            # transformers will throw a tantrum otherwise.  To attain\n",
        "            # the least possible memory usage, we create them as meta\n",
        "            # tensors, which don't take up any actual CPU or TPU memory.\n",
        "            if model_spec_key is None:\n",
        "                model_dict[key] = torch.empty(model_dict[key].shape, dtype=model_dict[key].dtype, device=\"meta\")\n",
        "                continue\n",
        "            \n",
        "            storage_key = model_dict[key].key\n",
        "            if storage_key != last_storage_key or model_dict[key].seek_offset < current_offset:\n",
        "                last_storage_key = storage_key\n",
        "                if isinstance(f, zipfile.ZipExtFile):\n",
        "                    f.close()\n",
        "                try:\n",
        "                    f = z.open(f\"archive/data/{storage_key}\")\n",
        "                except:\n",
        "                    f = z.open(f\"{zipfolder}/data/{storage_key}\")\n",
        "                current_offset = 0\n",
        "            if current_offset != model_dict[key].seek_offset:\n",
        "                f.read(model_dict[key].seek_offset - current_offset)\n",
        "                current_offset = model_dict[key].seek_offset\n",
        "            if not isinstance(model_dict[key], torch_lazy_loader.LazyTensor):\n",
        "                error = f\"Duplicate key {repr(key)}\"\n",
        "                print(\"\\n\\nERROR:  \" + error, file=sys.stderr)\n",
        "                raise RuntimeError(error)\n",
        "\n",
        "            \n",
        "            size = functools.reduce(lambda x, y: x * y, model_dict[key].shape, 1)\n",
        "            dtype = model_dict[key].dtype\n",
        "            nbytes = size if dtype is torch.bool else size * ((torch.finfo if dtype.is_floating_point else torch.iinfo)(dtype).bits >> 3)\n",
        "            tensor = model_dict[key].materialize(f, map_location=\"cpu\")\n",
        "            model_dict[key] = tensor.to(\"meta\")\n",
        "            current_offset += nbytes\n",
        "\n",
        "            # Transform by spec\n",
        "            spec = model_spec[model_spec_key]\n",
        "\n",
        "            transforms = set(spec.get(\"transforms\", ()))\n",
        "            if \"remove_first_two_rows\" in transforms:\n",
        "                tensor = tensor[2:]\n",
        "            if \"divide_by_shards\" in transforms:\n",
        "                tensor /= mtj_config[\"cores_per_replica\"]\n",
        "            if \"vocab_pad\" in transforms:\n",
        "                tensor = torch.nn.functional.pad(tensor, (0,) * (tensor.ndim * 2 - 1) + (mtj_config[\"n_vocab_padding\"],))\n",
        "\n",
        "            print(spec[\"module\"],spec[\"param\"])\n",
        "            old_shape = network.state[\"params\"][spec[\"module\"]][spec[\"param\"]].shape\n",
        "\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "\n",
        "            tensor = reshard_reverse(tensor, old_shape)\n",
        "\n",
        "            if np.isnan(tensor).any() or np.isinf(tensor).any():\n",
        "                raise ValueError(f\"bfloat16 overflow/underflow\")\n",
        "            assert tensor.shape == old_shape\n",
        "            tensor = jnp.array(tensor.detach())\n",
        "            \n",
        "            # network.state[\"params\"][spec[\"module\"]][spec[\"param\"]] = move_xmap(tensor, np.empty(cores_per_replica))\n",
        "            network.state[\"params\"][spec[\"module\"]][spec[\"param\"]] = move_xmap(tensor, np.empty(cores_per_replica))\n",
        "\n",
        "    print(f\"DONE file loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch_lazy_loader.use_lazy_torch_load(callback=callback, dematerialized_modules=True):\n",
        "    # torch_checkpoint = torch.load(path_to_checkpoint, map_location=\"cpu\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, cache_dir=\"cache\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for mk, mv in network.state[\"params\"].items():\n",
        "    for pk, pv in mv.items():\n",
        "        if isinstance(pv, PlaceholderTensor):\n",
        "            # The transformers GPT-J models apparently do not\n",
        "            # have embedding bias, whereas MTJ GPT-J models do,\n",
        "            # so we have to supplement an embedding bias tensor\n",
        "            # by creating a tensor with the necessary shape, filled\n",
        "            # with zeros.\n",
        "            if mk == \"causal_transformer_shard/~/embedding_shard/~/linear\" and pk == \"b\":\n",
        "                # mv[pk] = move_xmap(jnp.zeros(mv[pk].shape, dtype=jnp.bfloat16), np.empty(params[\"cores_per_replica\"]))\n",
        "                mv[pk] = move_xmap(jnp.zeros(mv[pk].shape, dtype=jnp.float32), np.empty(params[\"cores_per_replica\"]))\n",
        "\n",
        "            else:\n",
        "                error = f\"{mk} {pk} could not be found in the model checkpoint\"\n",
        "                print(\"\\n\\nERROR:  \" + error, file=sys.stderr)\n",
        "                raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# network = CausalTransformer(mtj_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "network.state = move_xmap(network.state, np.empty(cores_per_replica))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vwQOf2VX_S7"
      },
      "outputs": [],
      "source": [
        "def infer(context, top_p=0.9, temp=1.0, gen_len=512):\n",
        "    tokens = tokenizer.encode(context)\n",
        "\n",
        "    provided_ctx = len(tokens)\n",
        "    pad_amount = seq - provided_ctx\n",
        "\n",
        "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
        "    batched_tokens = np.array([padded_tokens] * total_batch)\n",
        "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
        "\n",
        "    start = time.time()\n",
        "    output = network.generate(\n",
        "        batched_tokens,\n",
        "        length,\n",
        "        gen_len,\n",
        "        {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp}\n",
        "    )\n",
        "\n",
        "    samples = []\n",
        "    decoded_tokens = output[1][0]\n",
        "    # print(\"output\", len(output[0]))\n",
        "    for o in decoded_tokens[:, :, 0]:\n",
        "      samples.append(tokenizer.decode(o))\n",
        "\n",
        "      #samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\n",
        "        # single = o[0][0, 0, seq : seq + gen_len]\n",
        "        # print(\"single\", single, tokenizer.decode(single))\n",
        "    print(f\"completion done in {time.time() - start:06}s\")\n",
        "    return samples\n",
        "\n",
        "# print(infer(\"EleutherAI is\")[0])\n",
        "# print(infer(\"EleutherAI is\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt2-maKhYBYN"
      },
      "outputs": [],
      "source": [
        "#@title  { form-width: \"300px\" }\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "#context = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\"\"\n",
        "context = \"\"\"The world of tomorrow is going\"\"\"\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=10, context=context))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Working example as of 2023/03/29 to connect transformers using accelerate to TPUs in colab with its limited RAM.\n",
        "\n",
        "REF:\n",
        "https://huggingface.co/docs/accelerate/v0.18.0/en/usage_guides/big_modeling#handling-big-models-for-inference\n",
        "https://huggingface.co/docs/accelerate/concept_guides/training_tpu "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers accelerate cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlnvJajiQC-W"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/sgugger/sharded-gpt-j-6B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MS2myrbWVMn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch_xla.core.xla_model as xm\n",
        "device = xm.xla_device()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtMQ_MWlQIfE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "from accelerate import Accelerator, init_empty_weights, load_checkpoint_and_dispatch\n",
        "import torch\n",
        "\n",
        "model_name_or_path = 'EleutherAI/gpt-j-6B'\n",
        "tokenizer_name = model_name_or_path\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "with init_empty_weights():\n",
        "   model = AutoModelForCausalLM.from_config(config)\n",
        "model.tie_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yysx5KB_QKK8"
      },
      "outputs": [],
      "source": [
        "model = load_checkpoint_and_dispatch(\n",
        "    model, \"sharded-gpt-j-6B\", device_map=\"balanced\", max_memory={\"xla:0\": \"7GiB\", \"xla:1\": \"7GiB\",\"xla:2\": \"7GiB\",\"xla:3\": \"7GiB\",\"xla:4\": \"7GiB\",\"xla:5\": \"7GiB\",\"xla:6\": \"7GiB\",\"xla:7\": \"7GiB\"}, offload_state_dict=True, no_split_module_classes=[\"GPTJBlock\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aOIXVwPQRFv"
      },
      "outputs": [],
      "source": [
        "input_text = \"Hi\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids.to(model.device), do_sample=True, max_length=10)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "print(\"Generated text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
